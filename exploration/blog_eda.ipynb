{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration\n",
    "\n",
    "Here we will do some quick EDA to determine what the plan will be \n",
    "to tackle the questions on the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "blog_df = pd.read_parquet('./data-science/blog/challenge-blog-00000.snappy.parquet')\n",
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.language.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Plan\n",
    "\n",
    "The main set of tasks are the following:\n",
    "- Analyze negative social media posts\n",
    "  - Prioritize negative topics \"Which fire to put out\"\n",
    "- Common topics between brand and competitors\n",
    "  - Differentiating factors between brand and competitors\n",
    "- Identify posts that implicitly reference brand\n",
    "- Identify trends before becoming obvious\n",
    "\n",
    "## ID trends before becoming obvious\n",
    "- Use sentiment analysis on title, text\n",
    "- Use NER, find most common brand\n",
    "- For each brand, find sentiment/emotions over time \n",
    "  - For explicit brands\n",
    "  - For implicit brands\n",
    "- Graph of brand's time-range vs num_negative_posts   \n",
    "\n",
    "## Some Considerations\n",
    "- Translate non-en to english\n",
    "- Use model for NER the body and title\n",
    "- Use model for Sentiment Analysis the body and title\n",
    "- Change date to datetime\n",
    "- Do topic modeling to get most common words\n",
    "\n",
    "\n",
    "# Limitations\n",
    "- Only English data will be considered due to computation and time constraints\n",
    "  - However, translation is a task that would take a while. Just for the titles, it took > 45min for a small model.add()\n",
    "- Sentiment Analysis and Emotion models have a limit of 512 characters. Normally I would chunk the data, and get an average of each piece.\n",
    "  - However, due to computation and time constraints, I will simply truncate the data \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Hugging Face Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranslationModel:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"alirezamsh/small100\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"alirezamsh/small100\")\n",
    "\n",
    "\n",
    "def translate_to_en(text):\n",
    "    TranslationModel.tokenizer.tgt_lang = \"en\" # type: ignore\n",
    "    encoded_hi = TranslationModel.tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = TranslationModel.model.generate(**encoded_hi)\n",
    "    translated_text = TranslationModel.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "sentence = \"hola como estan\"\n",
    "translate_to_en(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NERModel:\n",
    "    nlp = pipeline(\n",
    "        \"ner\", \n",
    "        model=AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\"), \n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\"), \n",
    "        grouped_entities=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "def get_ner_properties(text):\n",
    "    # Contains 4 entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\n",
    "    ner_results = NERModel.nlp(text)\n",
    "    return ner_results\n",
    "\n",
    "\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "get_ner_properties(text=example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SentimentTask:\n",
    "    model = pipeline(\n",
    "        \"sentiment-analysis\", \n",
    "        model=BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3), # type: ignore\n",
    "        tokenizer=BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\"),\n",
    "    ) \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmotionTask:\n",
    "    model = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class EmotionTaskLarge:\n",
    "    model = pipeline(\"text-classification\", model='SamLowe/roberta-base-go_emotions', return_all_scores=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Columns for Sentiment and Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract English and non-English texts first\n",
    "\n",
    "en_blogs = (blog_df.language == 'en')\n",
    "en_blog_df = blog_df.loc[en_blogs]\n",
    "non_en_blog_df = blog_df.loc[~en_blogs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_titles = en_blog_df.title.map(lambda title: get_ner_properties(title))\n",
    "ner_body = en_blog_df.title.map(lambda body: get_ner_properties(body))\n",
    "\n",
    "en_blog_df['ner_title'] = ner_titles\n",
    "en_blog_df['ner_body'] = ner_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_blog_df.publish_date = pd.to_datetime(en_blog_df.publish_date, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Takes around 25min to run\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "def truncate(text, max_tokens):\n",
    "    return text[:max_tokens]\n",
    "    \n",
    "en_blog_df = en_blog_df[(~en_blog_df.body.isna()) & (~en_blog_df.body.isna())]\n",
    "\n",
    "blog_title_sentiment = en_blog_df.title.map(lambda title: SentimentTask.model(truncate(title or '', max_tokens=MAX_TOKENS)))\n",
    "blog_title_emotion = en_blog_df.title.map(lambda title: EmotionTask.model(truncate(title or '', max_tokens=MAX_TOKENS)))\n",
    "\n",
    "blog_body_sentiment = en_blog_df.body.map(lambda title: SentimentTask.model(title[:MAX_TOKENS]))\n",
    "blog_body_emotion = en_blog_df.body.map(lambda title: EmotionTask.model(title[:MAX_TOKENS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_blog_df['title_sentiment'] = blog_title_sentiment\n",
    "en_blog_df['title_emotion'] = blog_title_emotion\n",
    "en_blog_df['body_sentiment'] = blog_body_sentiment\n",
    "en_blog_df['body_emotion'] = blog_body_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_blog_df.title_sentiment = en_blog_df.title_sentiment.map(lambda sentiment: sentiment[0]['label'])\n",
    "en_blog_df.body_sentiment = en_blog_df.body_sentiment.map(lambda sentiment: sentiment[0]['label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Brands\n",
    "\n",
    "The assignment doesn't give us the brand. However, we can look at the top brands that appear and keep it ambiguous such that we can recalculate \n",
    "some task for any subset of brands.\n",
    "\n",
    "To do this, we will use a Name Entity Recognition transfer learning model to give us the list of possible brands that exist in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_organizations(entities):\n",
    "    return [entity['word'] for entity in entities if entity['entity_group'] == 'ORG']\n",
    "\n",
    "en_blog_organizations = en_blog_df.ner_title.map(lambda entities: filter_organizations(entities))\n",
    "en_blog_organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_organizations = {}\n",
    "\n",
    "def add_organization(org, all_organizations):\n",
    "    if org not in all_organizations:\n",
    "        all_organizations[org] = 0\n",
    "    all_organizations[org] += 1\n",
    "    return None\n",
    "\n",
    "en_blog_organizations.map(lambda orgs: [add_organization(org, all_organizations) for org in orgs if org])\n",
    "\n",
    "organizations_with_most_counts_titles = sorted(all_organizations.items(), key=lambda x: -x[1])\n",
    "organizations_with_most_counts_titles[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get most common brands\n",
    "\n",
    "From the competitor brands found in the data, we can create a list that will help us compare and contrast\n",
    "in the future. We can just assume the brand and competitors for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAND = 'United Airlines'\n",
    "\n",
    "COMPETITOR_BRANDS = [\n",
    "    'Southwest Airlines',\n",
    "    'Spirit Airlines',\n",
    "    'American Airlines',\n",
    "    'United',\n",
    "    'Alaska Airlines',\n",
    "    'JetBlue'\n",
    "]\n",
    "\n",
    "IMPLICIT_IDENTIFIERS = [\n",
    "    'Airline',\n",
    "    'Airlines',\n",
    "    'General Aviation',\n",
    "    'FAA',\n",
    "    'Boeing'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Brand Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_brands(orgs):\n",
    "    brands = [org for org in orgs if org in COMPETITOR_BRANDS or org == BRAND or org in IMPLICIT_IDENTIFIERS]\n",
    "    return brands \n",
    "\n",
    "brands = en_blog_df.en_blog_organizations.map(lambda orgs: check_brands(orgs))\n",
    "en_blog_df['brands'] = brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_no_brands = en_blog_df['brands'].str.len() != 0\n",
    "blog_brands = en_blog_df[filter_no_brands]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Posts Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "class SentimentAnalysis:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentiment_graph(brands_df, feature, resample_by, strftime='%Y-%m-%d'):\n",
    "        timeseries_df = brands_df.set_index('publish_date')\n",
    "\n",
    "        negative_sentiment = timeseries_df[feature] == 'negative'\n",
    "        neutral_sentiment = timeseries_df[feature] == 'neutral'\n",
    "        positive_sentiment = timeseries_df[feature] == 'positive'\n",
    "\n",
    "        neg_trends = timeseries_df.loc[negative_sentiment, feature].resample(resample_by).count().reset_index()\n",
    "        neu_trends = timeseries_df.loc[neutral_sentiment, feature].resample(resample_by).count().reset_index()\n",
    "        pos_trends = timeseries_df.loc[positive_sentiment, feature].resample(resample_by).count().reset_index()\n",
    "\n",
    "        neg_trends['label'] = 'negative'\n",
    "        neu_trends['label'] = 'neutral'\n",
    "        pos_trends['label'] = 'positive'\n",
    "\n",
    "        all_labels_trends = pd.concat([neg_trends, neu_trends, pos_trends])\n",
    "\n",
    "        all_labels_trends = all_labels_trends.set_index('publish_date').reset_index()\n",
    "        all_labels_trends.publish_date = all_labels_trends.publish_date.dt.strftime(strftime)\n",
    "        \n",
    "        return all_labels_trends\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(df, title, x, y):\n",
    "        _, ax = plt.subplots(figsize = (10,4))\n",
    "\n",
    "        ax = sns.pointplot(x=x, y=y, data=df, hue='label', palette={'negative': 'red', 'neutral': 'gray', 'positive': 'green'})\n",
    "        ax.tick_params(axis='x', labelrotation = 45)\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        plt.show()\n",
    "        return None\n",
    "\n",
    "\n",
    "blog_title_sentiment_ts = SentimentAnalysis.sentiment_graph(brands_df=blog_brands, feature='title_sentiment', resample_by='H', strftime='%Y-%m-%d')\n",
    "SentimentAnalysis.plot(blog_title_sentiment_ts, title='Title Sentiment Analysis Over Time', x='publish_date', y='title_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_title_sentiment_ts = SentimentAnalysis.sentiment_graph(brands_df=blog_brands, feature='title_sentiment', resample_by='D', strftime='%Y-%m-%d')\n",
    "SentimentAnalysis.plot(blog_title_sentiment_ts, title='Title Sentiment Analysis Over Time', x='publish_date', y='title_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_title_sentiment_ts = SentimentAnalysis.sentiment_graph(brands_df=blog_brands, feature='body_sentiment', resample_by='D', strftime='%Y-%m-%d')\n",
    "SentimentAnalysis.plot(blog_title_sentiment_ts, title='Body Sentiment Analysis Over Time', x='publish_date', y='body_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_title_sentiment_ts = SentimentAnalysis.sentiment_graph(brands_df=blog_brands, feature='body_sentiment', resample_by='H', strftime='%Y-%m-%d')\n",
    "SentimentAnalysis.plot(blog_title_sentiment_ts, title='Body Sentiment Analysis Over Time', x='publish_date', y='body_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Topics\n",
    "\n",
    "- Within negative topics, get the emotions of title and body over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_brands = blog_brands[~blog_brands.body.isna()]\n",
    "negative_sentiment = blog_brands.body_sentiment == 'negative'\n",
    "positive_sentiment = blog_brands.body_sentiment == 'positive'\n",
    "\n",
    "\n",
    "negative_blogs = blog_brands[negative_sentiment]\n",
    "negative_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_blogs['title_emotion_lg'] = negative_blogs.title.map(lambda title: EmotionTaskLarge.model(title))\n",
    "negative_blogs['body_emotion_lg'] = negative_blogs.body.map(lambda body: EmotionTaskLarge.model(body[:MAX_TOKENS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_blogs.title_emotion_lg = negative_blogs.title_emotion_lg.map(lambda emotions: {d['label']:d['score'] for d in emotions[0]})\n",
    "negative_blogs.body_emotion_lg = negative_blogs.body_emotion_lg.map(lambda emotions: {d['label']:d['score'] for d in emotions[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(negative_blogs.title_emotion_lg[407].items(), key=lambda x: -x[1])\n",
    "negative_blogs_title_emotions = negative_blogs[['publish_date', 'title_emotion_lg']]\n",
    "\n",
    "negative_blogs_title_emotions.title_emotion_lg = negative_blogs_title_emotions.title_emotion_lg.map(lambda emotions: emotions.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_blogs_title_emotions = negative_blogs_title_emotions.explode('title_emotion_lg')\n",
    "negative_blogs_title_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue = negative_blogs_title_emotions.title_emotion_lg.map(lambda emotion: emotion[0])\n",
    "emotion_percents = negative_blogs_title_emotions.title_emotion_lg.map(lambda emotion: emotion[1])\n",
    "\n",
    "negative_blogs_title_emotions['hue'] = hue\n",
    "negative_blogs_title_emotions['emotion_percents'] = emotion_percents\n",
    "\n",
    "negative_blogs_title_emotions = negative_blogs_title_emotions.drop(columns='title_emotion_lg')\n",
    "negative_blogs_title_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_blogs_title_emotions_plt = negative_blogs_title_emotions.set_index('publish_date').groupby('hue').resample('D').mean().fillna(0).reset_index()\n",
    "negative_blogs_title_emotions_plt.publish_date = negative_blogs_title_emotions_plt.publish_date.dt.strftime('%Y-%m-%d')\n",
    "negative_blogs_title_emotions_plt.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_emotions = [\n",
    "    'anger', 'annoyance', 'confusion', 'disappointment',\n",
    "    'disapproval', 'disgust', 'embarrassment', 'fear',\n",
    "    'grief', 'nervousness', 'remorse', 'sadness',\n",
    "]\n",
    "\n",
    "emotions_filter = negative_blogs_title_emotions_plt.hue.isin(negative_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize = (20,6))\n",
    "\n",
    "ax = sns.pointplot(x='publish_date', y='emotion_percents', data=negative_blogs_title_emotions_plt[emotions_filter], hue='hue', )\n",
    "ax.tick_params(axis='x', labelrotation = 45)\n",
    "ax.set_title('Emotions Over Time')\n",
    "\n",
    "# plt.legend([],[], frameon=False)\n",
    "plt.legend(loc=\"upper left\", mode = \"expand\", ncol = 3) #\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand Sentiment Over Time\n",
    "- group brands, counts of sentiment over time for titles and body\n",
    "- match group with pos/neg sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "class BrandSentiment:\n",
    "    \n",
    "    @staticmethod\n",
    "    def brand_filtering(blog_brands, brands, feature):\n",
    "        brand_sentiment = blog_brands[[feature, 'brands', 'publish_date']]\n",
    "        brand_sentiment = brand_sentiment.rename(columns={feature: 'sentiment'})\n",
    "\n",
    "        brand_sentiment.brands = brand_sentiment.brands.map(lambda brand: brand[0])\n",
    "        brand_filter = brand_sentiment.brands.isin(brands)\n",
    "        brand_sentiment = brand_sentiment[brand_filter]\n",
    "        return brand_sentiment\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(brand_sentiment, by):\n",
    "        brand_sentiment = brand_sentiment[['publish_date', 'brands', 'sentiment']].set_index('publish_date')\n",
    "        brand_sentiment = brand_sentiment.groupby(['brands', 'sentiment']).resample(by).agg(count=('sentiment', 'count')).reset_index()\n",
    "        \n",
    "        brand_sentiment.publish_date = brand_sentiment.publish_date.dt.strftime('%Y-%m-%d')\n",
    "        return brand_sentiment\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(df, title, figsize=(10, 4)):\n",
    "        _, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        ax = sns.pointplot(x='publish_date', y='count', hue='sentiment', data=df, palette={'negative': 'red', 'neutral': 'gray', 'positive': 'green'})\n",
    "        ax.tick_params(axis='x', labelrotation = 45)\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        plt.show()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'title_sentiment', 'body_sentiment'\n",
    "brand_sentiment = BrandSentiment.brand_filtering(blog_brands, brands=COMPETITOR_BRANDS + [BRAND], feature='body_sentiment')\n",
    "brand_sentiment = BrandSentiment.resample(brand_sentiment, by='D')\n",
    "BrandSentiment.plot(brand_sentiment, title='All Brands Body Sentiment Over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'title_sentiment', 'body_sentiment'\n",
    "brand_sentiment = BrandSentiment.brand_filtering(blog_brands, brands=['United'], feature='body_sentiment')\n",
    "brand_sentiment = BrandSentiment.resample(brand_sentiment, by='D')\n",
    "BrandSentiment.plot(brand_sentiment, title='Brand Body Sentiment Over Time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'title_sentiment', 'body_sentiment'\n",
    "brand_sentiment = BrandSentiment.brand_filtering(blog_brands, brands=COMPETITOR_BRANDS, feature='body_sentiment')\n",
    "brand_sentiment = BrandSentiment.resample(brand_sentiment, by='D')\n",
    "BrandSentiment.plot(brand_sentiment, title='Competitor Brands Body Sentiment Over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_sentiment = BrandSentiment.brand_filtering(blog_brands, brands=IMPLICIT_IDENTIFIERS, feature='body_sentiment')\n",
    "brand_sentiment = BrandSentiment.resample(brand_sentiment, by='D')\n",
    "BrandSentiment.plot(brand_sentiment, title='Competitor Brands Body Sentiment Over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kona-takehome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
