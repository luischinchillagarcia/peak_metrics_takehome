{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration\n",
    "\n",
    "Here we will do some quick EDA to determine what the plan will be \n",
    "to tackle the questions on the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "blog_df = pd.read_parquet('./data-science/blog/challenge-blog-00000.snappy.parquet')\n",
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.language.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Plan\n",
    "\n",
    "The main set of tasks are the following:\n",
    "- Analyze negative social media posts\n",
    "  - Prioritize negative topics \"Which fire to put out\"\n",
    "- Common topics between brand and competitors\n",
    "  - Differentiating factors between brand and competitors\n",
    "- Identify posts that implicitly reference brand\n",
    "- Identify trends before becoming obvious\n",
    "\n",
    "## ID trends before becoming obvious\n",
    "- Use sentiment analysis on title, text\n",
    "- Use NER, find most common brand\n",
    "- For each brand, find sentiment/emotions over time \n",
    "  - For explicit brands\n",
    "  - For implicit brands\n",
    "- Graph of brand's time-range vs num_negative_posts   \n",
    "\n",
    "## Some Considerations\n",
    "- Translate non-en to english\n",
    "- Use model for NER the body and title\n",
    "- Use model for Sentiment Analysis the body and title\n",
    "- Change date to datetime\n",
    "- Do topic modeling to get most common words\n",
    "\n",
    "\n",
    "# Limitations\n",
    "- The original plan was to first translate the non-English data\n",
    "  - However, translation is a task that would take a while. Just for the titles, it took > 45min for a small model.add()\n",
    "  - Therefore I will filter the English lang values in this assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Hugging Face Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranslationModel:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"alirezamsh/small100\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"alirezamsh/small100\")\n",
    "\n",
    "\n",
    "def translate_to_en(text):\n",
    "    TranslationModel.tokenizer.tgt_lang = \"en\" # type: ignore\n",
    "    encoded_hi = TranslationModel.tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = TranslationModel.model.generate(**encoded_hi)\n",
    "    translated_text = TranslationModel.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "sentence = \"hola como estan\"\n",
    "translate_to_en(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NERModel:\n",
    "    nlp = pipeline(\n",
    "        \"ner\", \n",
    "        model=AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\"), \n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\"), \n",
    "        grouped_entities=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "def get_ner_properties(text):\n",
    "    # Contains 4 entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\n",
    "    ner_results = NERModel.nlp(text)\n",
    "    return ner_results\n",
    "\n",
    "\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "get_ner_properties(text=example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SentimentTask:\n",
    "    model = pipeline(\n",
    "        \"sentiment-analysis\", \n",
    "        model=BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3), # type: ignore\n",
    "        tokenizer=BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\"),\n",
    "    ) \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmotionTask:\n",
    "    model = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class EmotionTaskLarge:\n",
    "    model = pipeline(\"text-classification\", model='SamLowe/roberta-base-go_emotions', return_all_scores=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Columns for Sentiment and Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract English and non-English texts first\n",
    "\n",
    "en_blogs = (blog_df.language == 'en')\n",
    "en_blog_df = blog_df.loc[en_blogs]\n",
    "non_en_blog_df = blog_df.loc[~en_blogs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_titles = en_blog_df.title.map(lambda title: get_ner_properties(title))\n",
    "ner_body = en_blog_df.title.map(lambda body: get_ner_properties(body))\n",
    "\n",
    "en_blog_df['ner_title'] = ner_titles\n",
    "en_blog_df['ner_body'] = ner_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_blog_df.publish_date = pd.to_datetime(en_blog_df.publish_date, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 512\n",
    "\n",
    "def truncate(text, max_tokens):\n",
    "    return text[:max_tokens]\n",
    "    \n",
    "blog_title_sentiment = en_blog_df.title.map(lambda title: SentimentTask.model(truncate(title or '', max_tokens=MAX_TOKENS)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kona-takehome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
